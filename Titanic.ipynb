{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Let's import some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/titanic/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=pd.DataFrame()\n",
    "train_dataset=pd.read_csv('https://raw.githubusercontent.com/thousandoaks/AOMDeepLearningWorkshop/master/titanictrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We want to predict Survived, using the following features: PClass, Sex, Age, SibSp, Parch, Fare, Embarked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have to transform features Sex and Embarked (a.k.a label encoding) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.replace([\"male\", \"female\"], [0,1])\n",
    "train_dataset = train_dataset.replace([\"S\", \"C\", \"Q\"], [0,1,2])\n",
    "train_dataset= train_dataset.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train_dataset[[\"Survived\"]]\n",
    "features = train_dataset[[\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to numpy array for NN\n",
    "features = features.values\n",
    "target = target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target = to_categorical(train_dataset['Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "target[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will only use the following features: PClass, Age, SibSp,Parch, Fare\n",
    "#predictors=train_dataset[['Pclass','Sex','Age','SibSp','Parch','Fare']].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberofcolumns=features.shape[1]\n",
    "numberofcolumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Let's understand the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix=train_dataset.corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(correlation_matrix,linewidths=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Specify the architecture of the model (very basic one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "model.add(Dense(100,activation='relu',input_shape=(numberofcolumns,)))\n",
    "\n",
    "# Add a second layer\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Let's compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model. Add accuracy as the metric to benchmark our model\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Let's fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fit the model. This time we keep 20% of our samples for test purposes\n",
    "model_training=model.fit(features,target,verbose=2,epochs=100,validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Let's evaluate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "pyplot.plot(model_training.history['val_loss'], 'r')\n",
    "pyplot.xlabel('Epochs')\n",
    "pyplot.ylabel('Validation score')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Let's improve our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the model\n",
    "#model2 = Sequential()\n",
    "# Add the first layer\n",
    "#model2.add(Dense(100,activation='relu',input_shape=(numberofcolumns,)))\n",
    "# Add a second layer\n",
    "\n",
    "\n",
    "#model2.add(Dense(100, activation='relu'))\n",
    "#model2.add(Dense(10, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "#model2.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "#input layer\n",
    "model2.add(Dense(100, input_shape=(numberofcolumns,)))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation(\"relu\"))\n",
    "model2.add(Dropout(0.4))\n",
    "\n",
    "# hidden layers\n",
    "model2.add(Dense(50))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation(\"sigmoid\"))\n",
    "model2.add(Dropout(0.4))\n",
    "    \n",
    "model2.add(Dense(10))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation(\"sigmoid\"))\n",
    "model2.add(Dropout(0.4))\n",
    "    \n",
    "#model2.add(Dense(2, activation=\"sigmoid\"))\n",
    "    \n",
    "# output layer\n",
    "model2.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model. Add accuracy as the metric to benchmark our model\n",
    "model2.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Fit the model. This time we keep 20% of our samples for test purposes\n",
    "model2_training=model2.fit(features,target,verbose=2,epochs=100,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "pyplot.plot(model2_training.history['val_loss'], 'r',label='performance model 2 on test data')\n",
    "pyplot.plot(model_training.history['val_loss'], 'b',label='performance model 1 on test data')\n",
    "pyplot.xlabel('Epochs')\n",
    "pyplot.ylabel('Validation score')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 seems to be performing better in the long run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is model 2 performing ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset['Predicted']=pd.DataFrame(model2.predict(features))\n",
    "# we transform probabilities into discrete outcomes (0/1)\n",
    "train_dataset['Predicted_Discrete']=round(train_dataset['Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "\n",
    "\n",
    "confusion_matrix=pd.crosstab(train_dataset['Survived'], train_dataset['Predicted_Discrete'], rownames=['Actual Types'], colnames=['Predicted Types'])\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We observe that model 2 is relatively good at predicting who will perish.\n",
    "### Moreover model 2 achieves 83% of accuracy (is it good enough ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### However model 2 predicted many survivors which did not survive in practice (62). \n",
    "### Model 2 predicted many fatalities which did survive in practice (98). \n",
    "\n",
    "#### This gives us some hints for further improvement. Manual Analysis of misclassifications to understand the causes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
